<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=1200">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
  <!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> -->
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */

    .button {
    display: inline-block;
    padding: 1px 20px;
    text-align: center;
    text-decoration: none;
    color: 1772d0;
    background-color: white;
    border: 2px solid #1772d0;;
    border-radius:20px;
    padding: 1px 10px;
    font-size: 12px;
    /* box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1); */
    cursor: pointer;
    transition: background-color 0.3s;
  }

  .button:hover {
    background-color: #5a6092;
  }
  
    .container {
    display: flex; /* 使用flexbox布局 */
    /* align-items: center;  */
    /* vertical-align: top; */
    }


    .icon {
      background-color: #bc85d3; /* 调整颜色以匹配准确的色调 */
      color: white;
      font-size: 14px;
      font-family: Arial, sans-serif;
      padding: 6px 15px; /* 调整适当的间距 */
      border-radius: 10px; /* 这提供了药丸形状 */
      text-align: center;
      margin-right: 20px;
      margin-left: 0px;
      margin-top: 0px;
      box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1); /* 添加阴影 */
      height: 27px;
}



      nav {
          background-color: #333;
          color: #fff;
          padding: 10px 0;
          text-align: center;
      }
      
      nav a {
          text-decoration: none;
          color: #fff;
          font-weight: bold;
          padding: 10px 20px;
          margin: 0 15px;
          border-radius: 5px;
          transition: background-color 0.3s ease;
      }
      
      nav a:hover {
          background-color: #555;
      }

    a {
      color: #1772d0;
      text-decoration: none;
    }

    
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 17px,
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 17px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 24px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 17px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 34px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    .grid-container {
      display: grid;
      grid-template-columns: repeat(2, 1fr);
      grid-gap: 20px;
    }
    .text-block {
      border: 1px solid #ddd;

      padding: 10px;
      text-align: center;
    }
    span.highlight {
      background-color: #ffffd0;
    }
    
  </style>
  <link rel="icon" type="image/png" href="images/572.png">
  <title>Tianxin Wei</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <!-- 目录栏 -->
  <nav>
    <a href="#section1">Bio</a>
    <a href="#section2">Education</a>
    <a href="#section3">Work Experience</a>
    <a href="#section4">News</a>
    <a href="#section5">Publications </a>
    <a href="#section6">Services & Awards</a>
  </nav>
  <table width="1000" border="0" align="center" cellspacing="0" cellpadding="0" id="section1">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="70%" valign="middle">
              <p align="center">
                <name>Tianxin Wei</name>
              </p>
              <p>I am a final-year Ph.D. student advised by <a href="https://www.hejingrui.org/"> Prof. Jingrui He </a> at the University of Illinois Urbana-Champaign.   
                Prior to joining UIUC in 2021, I earned my B.S. degree in Computer Science from University of Science and Technology of China, School of the Gifted Young. 
              </p>
                 
              </p>
              <p>
                My research primarily centers on enhancing the personalization and efficiency of LLMs across various modalities and disciplines, with the ultimate goal of making ML models more adaptive and accessible. My research interests covers a wide range of topics:
<ul>
                  <li><strong>LLM Personalization</strong>: user modeling, generative recommendation, and trustworthy adaptation;</li>
                  <li><strong>LLM Agents</strong>: long-term reasoning, self-evolving memory, and interaction-driven learning;</li>
                  <li><strong>LLM Efficiency</strong>: sampling, routing, and scalable MoE/Transformer optimization.</li>
                  <li><strong>Applications</strong>: multidisciplinary and multimodal applications spanning agriculture, recommender systems, and scientific domains.</li>
                </ul>
          
<!--     <li><strong>Knowledge-enhanced LLM</strong>: content/KG retrieval and knowledge fusion; </li>
                  <li><strong>LLM Governance/Policy</strong>: technical strategies for implementing regulatory principles effectively;</li> -->
                Feel free to drop me an e-mail if you are interested in my research and want to discuss relevant research topic or potential collaborations!
              </p>
              <p align=center>
                <a href="https://weitianxin.github.io/files/CV_wtx.pdf">CV</a> &nbsp/&nbsp
                <a href="mailto:twei10@illinois.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=_LU2-kMAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/weitianxin">Github</a> &nbsp/&nbsp
                <a href="https://twitter.com/wei_tianxin">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/tianxin-wei-7063a2180/">Linkedin</a> &nbsp
              </p>
            </td>
            <td width="30%">
              <img src="images/wtx_mori_2.jpg" class="img-fluid z-depth-1 rounded-circle" width="100%">
            </td>
          </tr>
        </table>
        


        
        <!-- Education -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" id="section2">
          <tbody><tr>
            <td width="100%" valign="middle">
              <heading>Education</heading>
              <hr>
              <table>
                <tbody>

<!-- Ph.D. Section -->
<tr style="border-bottom: 1px solid #ccc; background-color: #f9f9f9;">
    <td width="100%" style="padding: 15px; vertical-align: top;">
        <b>Ph.D.</b> &nbsp; 2021 - 2026 (expected) <br>
        University of Illinois Urbana-Champaign (UIUC) <br>
        Advisor: <a href="https://www.hejingrui.org/">Prof. Jingrui He</a>
    </td>
    <td width="30%" style="padding: 15px; text-align: center;">
        <img src="images/uiuc.png" width="90px" style="border-radius: 8px;">
    </td>
</tr>

<!-- B.S. Section -->
<tr style="background-color: #ffffff;">
    <td width="100%" style="padding: 15px; vertical-align: top;">
        <b>B.S.</b> &nbsp; 2016 - 2020 <br>
        University of Science and Technology of China (USTC) <br>
        School of the Gifted Young <br>
        Advisor: <a href="https://hexiangnan.github.io/">Prof. Xiangnan He</a>
    </td>
    <td width="30%" style="padding: 15px; text-align: center;">
        <img src="images/ustc.png" width="90px" style="border-radius: 8px;">
    </td>
</tr>
</tbody>
              </table>
            </td>
          </tr>
          </tbody></table>
    
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" id="section3">
            <tbody><tr>
              <td width="100%" valign="middle">
              <heading>Work Experience</heading>
              <hr>
              <table>
                <tbody>
                  
<tr style="border-bottom: 1px solid #ccc; background-color: #ffffff;">
  
     <td width="100%" style="padding: 15px; vertical-align: top;">
        <b>Google DeepMind GenAI</b>, Mountain View, CA<br>
        <i>Intern • May-Aug 2025</i><br>
        <i>LLM Agent Memory and Personalization</i>
    </td>
  <td width="30%" style="padding: 15px; text-align: center;">
        <img src="images/DeepMind_new_logo.svg.png" width="130px" style="border-radius: 8px;">
    </td>
</tr>
                 
<tr style="border-bottom: 1px solid #ccc; background-color: #f9f9f9;">
  
    <td width="100%" style="padding: 15px; vertical-align: top;">
        <b>Meta Ranking AI</b>, Urbana, IL (Remote)<br>
        <i>Intern • Jan-May 2025</i><br>
        <i>Generative Recommender Systems with LLMs</i>
    </td>
    <td width="30%" style="padding: 15px; text-align: center;">
        <img src="images/Meta-Logo.png" width="120px" style="border-radius: 8px;">
    </td>
</tr>

<tr style="border-bottom: 1px solid #ccc; background-color: #ffffff;">
    <td width="100%" style="padding: 15px; vertical-align: top;">
        <b>Amazon Shopping</b>, Palo Alto, CA and Urbana, IL<br>
        <i>Intern • May-Oct 2024</i><br>
        <i>Multi-modal Watermarking for Generative Models</i>
    </td>
    <td width="30%" style="padding: 15px; text-align: center;">
        <img src="images/amazon.png" width="110px" style="border-radius: 8px;">
    </td>
</tr>

<tr style="background-color: #f9f9f9;">
    <td width="100%" style="padding: 15px; vertical-align: top;">
        <b>Amazon Search</b>, Palo Alto, CA and Urbana, IL<br>
        <i>Intern • May-Dec 2023</i><br>
        <i>Multi-modal Large Language Models for Personalization</i><br>
        <i>Four papers published at ICLR'24, WWW'24 (Oral), ICML'24, and KDD'25.</i>
    </td>
    <td width="30%" style="padding: 15px; text-align: center;">
        <img src="images/amazon.png" width="110px" style="border-radius: 8px;">
    </td>
</tr>
</tbody>


            </table>
          </td>
        </tr>
        </tbody></table>

                        <!--                           Main Advisors: <a href="https://scholar.google.com/citations?user=u1PEv-QAAAAJ&hl=zh-CN"> Dr. Xianfeng Tang </a> at Amazon; <a href="https://suhangwang.ist.psu.edu/"> Prof. Suhang Wang</a> at PSU </a><br>-->

<!--                           Main Advisors: <a href="https://scholar.google.com/citations?user=u1PEv-QAAAAJ&hl=zh-CN"> Dr. Xianfeng Tang </a> at Amazon; <a href="https://suhangwang.ist.psu.edu/"> Prof. Suhang Wang</a> at PSU </a> <br>
                       -->

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" id="section4">
          <tbody><tr>
            <td width="100%" valign="middle">
              <heading>News</heading>
              <hr>
              <div class="table-responsive" style="max-height: 9.5vw;">
                <table class="table table-sm table-borderless" >
                  <tr>
                    <th scope="row">Nov, 2025</th>
                    <td>
                         One paper accepted @ AAAI'26.           
                    </td>
                  </tr>
                  <tr>
                    <th scope="row">Aug, 2025</th>
                    <td>
                         One research and workshop paper accepted @ NeurIPS'25.           
                    </td>
                  </tr>
                  <tr>
                    <th scope="row">Aug, 2025</th>
                    <td>
                         One paper accepted @ EMNLP'25.           
                    </td>
                  </tr>
                  <tr>
                    <th scope="row">Aug, 2025</th>
                    <td>
                         One paper accepted @ CIKM'25.           
                    </td>
                  </tr>
                  <tr>
                    <th scope="row">June, 2025</th>
                    <td>
                         One paper accepted @ ICCV'25.           
                    </td>
                  </tr> 
                  <tr>
                    <th scope="row">May, 2025</th>
                    <td>
                         One paper accepted @ ACL'25.           
                    </td>
                  </tr> 
                  <tr>
                    <th scope="row">May, 2025</th>
                    <td>
                         One paper accepted @ UAI'25.           
                    </td>
                  </tr> 
                  <tr>
                    <th scope="row">May, 2025</th>
                    <td>
                         Two papers accepted @ ICML'25.           
                    </td>
                  </tr> 
                  <tr>
                    <th scope="row">Apr, 2025</th>
                    <td>
                         Will join Google DeepMind as an intern this summer. See you in Mountain View.              
                    </td>
                  </tr> 
                  <tr>
                    <th scope="row">Dec, 2024</th>
                    <td>
                         Recognized as the Outstanding Reviewer @ KDD’25.              
                    </td>
                  </tr> 
                  <tr>
                    <th scope="row">Nov, 2024</th>
                    <td>
                         Two (co-)first-author papers accepted @ KDD’25.              
                    </td>
                  </tr> 
                  <tr>
                    <th scope="row">May, 2024</th>
                    <td>
                         One paper accepted @ KDD’24.              
                    </td>
                  </tr> 
                  <tr>
                    <th scope="row">May, 2024</th>
                    <td>
                         Two papers accepted @ ICML’24.              
                    </td>
                  </tr> 
                  <tr>
                    <th scope="row">Jan, 2024</th>
                    <td>
                         One paper <a href="https://arxiv.org/abs/2311.09134">RIPOR</a> (Scalable and effective generative retrieval) accepted @ WWW’24.              
                    </td>
                  </tr> 

                  <tr>
                    <th scope="row">Jan, 2024</th>
                    <td>
                      One paper <a href="https://openreview.net/forum?id=khAE1sTMdX" style="color: #1772d0">UniMP</a> (multi-modal personalization including recommendation and search, etc.) accepted @ ICLR’24.              
                    </td>
                  </tr> 

                  <tr>
                    <th scope="row">Dec, 2023</th>
                    <td>
                      One paper accepted @ AAAI’24. Will attend NeurIPS'23 between Dec. 9-16. See you there!              
                    </td>
                  </tr> 

                  <tr>
                    <th scope="row">Oct, 2023</th>
                    <td>
                      Awarded the NeurIPS’23 Scholar Award. Thanks to NeurIPS!          
                    </td>
                  </tr> 

                  <tr>
                    <th scope="row">Sep, 2023</th>
                    <td>
                      Two papers (Test-time personalization and bandit scheduler for meta-learning) accepted @ NeurIPS’23.         
                    </td>
                  </tr> 

             

                  <tr>
                    <th scope="row">May, 2023</th>
                    <td>
                      Received the ICML’23 Grant Award. Thanks to ICML!         
                    </td>
                  </tr> 

                  <tr>
                    <th scope="row">Apr, 2023</th>
                    <td>
                      <a href="https://weitianxin.github.io/files/mlp fusion.pdf" style="color: #1772d0">MLP Fusion</a> (NTK-approximating MLP Fusion for efficient learning) accepted @ ICML’23.          
                    </td>
                  </tr> 

                  <tr>
                    <th scope="row">Mar, 2023</th>
                    <td>
                      Will join Amazon Search team as an applied scientist intern this summer. See you in Palo Alto!         
                    </td>
                  </tr> 

                  <tr>
                    <th scope="row">Dec, 2022</th>
                    <td>
                      Collect a curated list of papers on the distribution shift. Check out at <a href="https://github.com/weitianxin/awesome-distribution-shift" style="color: #1772d0">awesome-distribution-shift</a>!        
                    </td>
                  </tr> 

                  <tr>
                    <th scope="row">Oct, 2022</th>
                    <td>
                      Awarded the NeurIPS’22 Scholar Award. Thanks to NeurIPS!          
                    </td>
                  </tr> 

                  <tr>
                    <th scope="row">Oct, 2022</th>
                    <td>
                      <a href="https://arxiv.org/abs/2210.03801" style="color: #1772d0">HyperGCL</a> (contrastive learning on hypergraphs) accepted @ NeurIPS’22.         
                    </td>
                  </tr> 

                  <tr>
                    <th scope="row">May, 2022</th>
                    <td>
                      <a href="https://arxiv.org/abs/2206.04789" style="color: #1772d0">CLOVER</a> (comprehensive fairness of cold-start recsys) accepted @ KDD’22.         
                    </td>
                  </tr> 

                  <tr>
                    <th scope="row">Jul, 2021</th>
                    <td>
                      Awarded the SIGIR 2021 Best Paper Honorable Mention. Thanks to SIGIR!         
                    </td>
                  </tr> 

                  <tr>
                    <th scope="row">May, 2021</th>
                    <td>
                      <a href="https://arxiv.org/abs/2010.15363" style="color: #1772d0">MACR</a> (popularity debias with causal counterfactual reasoning) accepted @ KDD’21.         
                    </td>
                  </tr> 

                  <tr>
                    <th scope="row">Apr, 2021</th>
                    <td>
                      <a href="https://arxiv.org/abs/2105.06067" style="color: #1772d0">PDA</a> (popularity adjusted deconfounded training with causal intervention) accepted @ SIGIR’21.         
                    </td>
                  </tr> 

                  <tr>
                    <th scope="row">Aug, 2020</th>
                    <td>
                      <a href="https://weitianxin.github.io/files/ICDM_2020_MetaCF.pdf" style="color: #1772d0">MetaCF</a> (graph meta-learning for cold-start recsys) accepted @ ICDM’20. Internship work at <a href="https://web.cs.ucla.edu/~yzsun/">UCLA</a>.         
                    </td>
                  </tr> 
                </table>
            </tr>
          </tbody></table>

      
        <!-- research -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" id="section5">
          <tr>
            <td width="100%" valign="middle">
              <heading>Publications (*equal contribution ^mentorship)</heading>
              <hr>
              &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<strong>Preprint</strong>
            </td>
             
          </tr>
        </table>
         

        <ul>

          <li>
            <a href="https://arxiv.org/abs/2511.20857"><strong>Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory</strong></a>
            <p>
                <u>Tianxin Wei</u>, Noveen Sachdeva, Benjamin Coleman, Zhankui He, Yuarchen Bei, 
                Xuying Ning, Mengting Ai, Yunzhe Li, Jingrui He, Ed H. Chi, Chi Wang, Shuo Chen, 
                Fernando Pereira, Wang-Cheng Kang, Derek Zhiyuan Cheng
            </p>
            <p>
                <em>arXiv 2025.</em> 
                <a href="TBD" class="button">Code</a>
            </p>
        </li>

          <li>
            <a href="TBD"><strong>MC-Search: Evaluating and Enhancing Multimodal Agentic Search with Structured Long Reasoning Chains</strong></a>
            <p>Xuying Ning*, Dongqi Fu*, <u>Tianxin Wei</u>*, Mengting Ai, Jiaru Zou, Ting-Wei Li, Hanghang Tong, Yada Zhu, Hendrik Hamann, Jingrui He</p>
            <p><em>Under Review. NeurIPS 2025 Workshop on on Evaluating the Evolving LLM Lifecycle.</em> <a href="TBD" class="button">Code</a></p>
            <img src="images/tag-multimodal.png" width="9%"> <img src="images/tag-robustness.png" width="14.5%">
          </li>

          <li>
              <a href="https://www.arxiv.org/abs/2511.22707"><strong>CoFiRec: Coarse-to-Fine Tokenization for Generative Recommendation</strong></a>
              <p>
                  <u>Tianxin Wei*</u>, Xuying Ning*, Xuxing Chen, Ruizhong Qiu, Yupeng Hou, 
                  Yan Xie, Shuang Yang, Zhigang Hua, Jingrui He
              </p>
              <p>
                  <em>arXiv 2025.</em> 
                  <a href="TBD" class="button">Code</a>
              </p>
              <img src="images/tag-application.png" width="11%"> <img src="images/tag-multimodal.png" width="9%">
          </li>

          <li>
            <a href="TBD"><strong>Inference Scaling of LLM Ensembling: Bridging Token Spaces with Token Translation</strong></a>
            <p>
                <u>Tianxin Wei</u>, Zhichen Zeng, Ruizhong Qiu, Zhining Liu, Xuying Ning, 
                Xinrui He, Wenxuan Bao, Qi He, Xianfeng Tang, Hanghang Tong, Jingrui He
            </p>
            <p>
                <em>Under Review.</em> 
                <a href="TBD" class="button">Code</a>
            </p>
            <img src="images/tag-efficiency.png" width="14%">
        </li>

        <li>
            <a href="TBD"><strong>DiffKGW: Stealthy and Robust Diffusion Model Watermarking</strong></a>
            <p>
                <u>Tianxin Wei</u>, Ruizhong Qiu, Yifan Chen, Yunzhe Qi, Jiacheng Lin, Wenju Xu, 
                Sreyashi Nag, Ruirui Li, Hanqing Lu, Zhengyang Wang, Chen Luo, Hui Liu, 
                Suhang Wang, Jingrui He, Qi He, Xianfeng Tang
            </p>
            <p>
                <em>Under submission.</em> 
                <a href="TBD" class="button">Code</a>
            </p>
            <img src="images/tag-robustness.png" width="14%"> 
        </li>

          
          <!-- <li>
            <a href="TBD"><strong>Robust Watermarking for Diffusion Models</strong></a>
            <p><u>Tianxin Wei</u>, etc.</p>
            <p><em>preprint 2024.</em> <a href="TBD" class="button">Code</a></p>
            <img src="images/tag-safety.png" width="13.5%">

          <li>
            <a href="https://arxiv.org/pdf/2410.06467"><strong>WAPITI: A Watermark for Finetuned Open-Source LLMs</strong></a>
            <p>Lingjie Chen, Ruizhong Qiu, Siyu Yuan, Zhining Liu, <u>Tianxin Wei</u>, Hyunsik Yoo, Zhichen Zeng, Deqing Yang, Hanghang Tong</p>
            <p><em>preprint 2024.</em> <a href="TBD" class="button">Code</a></p>
            <img src="images/tag-safety.png" width="13.5%">

          <li>
            <a href="https://arxiv.org/pdf/2410.11235"><strong>Unleashing the Power of LLMs as Multi-Modal Encoders for Text and Graph-Structured Data</strong></a>
            <p>Jiacheng Lin, Kun Qian, Haoyu Han, Nurendra Choudhary, <u>Tianxin Wei</u>, Zhongruo Wang, Sahika Genc, Edward W Huang, Sheng Wang, Karthik Subbian, Danai Koutra, Jimeng Sun</p>
            <p><em>preprint 2024.</em> <a href="TBD" class="button">Code</a></p>
            <img src="images/tag-multimodal.png" width="9%"> -->
        </ul>

        <!-- <div class="container">
          <div class="icon">NeurIPS</div>
          <div><a href="https://openreview.net/forum?id=khAE1sTMdX"><strong>Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond</strong></a>
            <p><u>Tianxin Wei</u>, Bowen Jin, Ruirui Li, Hansi Zeng, Zhengyang Wang, Jianhui Sun, Qingyu Yin, Hanqing Lu, Suhang Wang, Jingrui He, Xianfeng Tang</p>
            <p>In the ICLR 2024 (Full Research, AR: 31%). [<a href="TBD">Code</a>]</p>
            <img src="images/tag-multimodal.png" width="9%"> <img src="images/tag-transferability.png" width="15%"></div>
        </div> -->

        &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<strong>Conference</strong>
        <br>
        <br>
  
        <ul>
          <li>
            <a href=""><strong>Panda: Test-Time Adaptation with Negative Data Augmentation</strong></a>
            <p>Ruxi Deng, Wenxuan Bao, <u>Tianxin Wei</u>, Jingrui He</p>
            <p><em>AAAI 2026.</em> <a href="TBD" class="button">Code</a></p>
            <img src="images/tag-robustness.png" width="14.5%">
          </li>

          <li>
            <a href="https://arxiv.org/abs/2505.17451"><strong>CLIMB: Class-imbalanced Learning Benchmark on Tabular Data</strong></a>
            <p>Zhining Liu, Zihao Li, Ze Yang, <u>Tianxin Wei</u>, Jian Kang, Yada Zhu, Hendrik Hamann, Jingrui He, Hanghang Tong</p>
            <p><em>NeurIPS 2025.</em> <a href="TBD" class="button">Code</a></p>
            <img src="images/tag-robustness.png" width="14.5%">
          </li>
          
          <li>
            <a href="https://arxiv.org/abs/2507.21494"><strong>Latte: Collaborative Test-Time Adaptation of Vision-Language Models in Federated Learning</strong></a>
            <p>Wenxuan Bao, Ruxi Deng, Ruizhong Qiu, <u>Tianxin Wei</u>, Hanghang Tong, Jingrui He</p>
            <p><em>ICCV 2025.</em> <a href="TBD" class="button">Code</a></p>
            <img src="images/tag-multimodal.png" width="9%"> <img src="images/tag-robustness.png" width="14.5%">
          </li>
          
          <li>
            <a href="TBD"><strong>Learning to Instruct: Fine-Tuning a Task-Aware Instruction Optimizer for Black-Box LLMs</strong></a>
            <p>Yunzhe Qi, Jinjin Tian, Tianci Liu, Ruirui Li, <u>Tianxin Wei</u>, Hui Liu, Xianfeng Tang, Monica Xiao Cheng, Jingrui He</p>
            <p><em>EMNLP 2025. (Findings)</em> <a href="TBD" class="button">Code</a></p>
            <img src="images/tag-sample_efficiency.png" width="12.5%">
          </li>

          <!-- <li>
            <a href="TBD"><strong>PyG-SSL: A Graph Self-Supervised Learning Toolkit</strong></a>
            <p>Lecheng Zheng, Baoyu Jing, Zihao Li, Zhichen Zeng, <u>Tianxin Wei</u>, Mengting Ai, Xinrui He, Lihui Liu, Dongqi Fu, Jiaxuan You, Hanghang Tong, Jingrui He</p>
            <p><em>CIKM 2025.</em> <a href="TBD" class="button">Code</a></p>
            <img src="images/tag-robustness.png" width="14.5%">
        </li> -->
          
          <li>
            <a href="https://arxiv.org/abs/2502.08767"><strong>SelfElicit: Your Language Model Secretly Knows Where is the Relevant Evidence</strong></a>
            <p>Zhining Liu, Rana Ali Amjad, Ravinarayana Adkathimar, <u>Tianxin Wei</u>, Hanghang Tong</p>
            <p><em>ACL 2025. (Main)</em> <a href="TBD" class="button">Code</a></p>
            <img src="images/tag-retrieval.png" width="13%">
          </li>
          
          <li>
            <a href="https://arxiv.org/abs/2405.20710"><strong>i<sup>2</sup>VAE: Interest Information Augmentation with Variational Regularizers for Cross-Domain Sequential Recommendation</strong></a>
            <p>Xuying Ning, Wujiang Xu, <u>Tianxin Wei</u>, Xiaolei Liu</p>
            <p><em>UAI 2025.</em> <a href="TBD" class="button">Code</a></p>
            <img src="images/tag-application.png" width="11%"> <img src="images/tag-robustness.png" width="14.5%">
          </li>
          
          <li>
            <a href="https://openreview.net/forum?id=FB2e8PV6qg"><strong>Graph4MM: Weaving Multimodal Learning with Structural Information</strong></a>
            <p>Xuying Ning, Dongqi Fu, <u>Tianxin Wei</u>, Wujiang Xu, Jingrui He</p>
            <p><em>ICML 2025.</em> <a href="TBD" class="button">Code</a></p>
            <img src="images/tag-multimodal.png" width="9%">
          </li>
          
          <li>
            <a href="https://arxiv.org/abs/2505.18442"><strong>Breaking Silos: Adaptive Model Fusion Unlocks Better Time Series Forecasting</strong></a>
            <p>Zhining Liu, Ze Yang, Xiao Lin, Ruizhong Qiu, <u>Tianxin Wei</u>, Yada Zhu, Hendrik Hamann, Jingrui He, Hanghang Tong</p>
            <p><em>ICML 2025.</em> <a href="TBD" class="button">Code</a></p>
            <img src="images/tag-application.png" width="11%"> <img src="images/tag-model_efficiency.png" width="13.5%">
          </li>


          <li>
            <a href="TBD"><strong>Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization</strong></a>
            <p><u>Tianxin Wei</u>*, Yifan Chen*, Wenxuan Bao, Jingrui He</p>
            <p><em>KDD 2025. (Full Research, AR: 19%).</em> <a href="TBD" class="button">Code</a></p>
            <img src="images/tag-robustness.png" width="14.5%">
          </li>

          <li>
            <a href="https://arxiv.org/abs/2307.08941"><strong>MLP Fusion: Towards Efficient Fine-tuning of Dense and Mixture-of-Experts Language Models</strong></a>
            <p>Mengting Ai*, <u>Tianxin Wei</u>*, Yifan Chen*, Zeming Guo, Jingrui He</p>
            <p><em>v1 accepted to ICML 2023. This work has been submitted to the IEEE for possible publication.</em> <a href="TBD" class="button">Code</a></p>
            <img src="images/tag-model_efficiency.png" width="13.5%">
          
          <li>
            <a href="TBD"><strong>ResMoE: Space-efficient Compression of Mixture of Experts LLMs via Residual Restoration</strong></a>
            <p>Mengting Ai*, <u>Tianxin Wei</u>*^, Yifan Chen*, Zhichen Zeng, Ritchie Zhao, Girish Varatkar, Bita Darvish Rouhani, Xianfeng Tang, Hanghang Tong, Jingrui He</p>
            <p><em>KDD 2025. (Full Research, AR: 19%).</em> <a href="TBD" class="button">Code</a></p>
            <img src="images/tag-model_efficiency.png" width="12.5%">
          </li>

          
          
          <li>
            <a href="https://arxiv.org/abs/2201.13395"><strong>Meta Clustering of Neural Bandits</strong></a>
            <p>Yikun Ban, Yunzhe Qi, <u>Tianxin Wei</u>, Lihui Liu, Jingrui He</p>
            <p><em>KDD 2024. (Full Research, AR: 20%).</em> <a href="TBD" class="button">Code</a></p>
            <img src="images/tag-sample_efficiency.png" width="12.5%">
        </li>    
          
          <li>
            <a href="TBD"><strong>Graph Mixup on Approximate Gromov–Wasserstein Geodesics</strong></a>
            <p>Zhichen Zeng, Ruizhong Qiu, Zhe Xu, Zhining Liu, Yuchen Yan, <u>Tianxin Wei</u>, Lei Ying, Jingrui He, Hanghang Tong</p>
            <p><em>ICML 2024 (Full Research, AR: 27.5%).</em> <a href="TBD" class="button">Code</a></p>
            <img src="images/tag-robustness.png" width="13%">
          </li>
          
          <li>
            <a href="https://arxiv.org/pdf/2310.07815v1.pdf"><strong>Language Models as Semantic Indexers</strong></a>
            <p>Bowen Jin, Hansi Zeng, Guoyin Wang, Xiusi Chen, <u>Tianxin Wei</u>, Ruirui Li, Zhengyang Wang, Zheng Li, Yang Li, Hanqing Lu, Suhang Wang, Jiawei Han, Xianfeng Tang</p>
            <p><em>ICML 2024 (Full Research, AR: 27.5%).</em> <a href="https://github.com/petergriffinjin/lmindexer" class="button">Code</a></p>
            <img src="images/tag-retrieval.png" width="13%">
          </li>
          
          <li> <a href="https://openreview.net/pdf?id=khAE1sTMdX"><strong>Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond</strong></p></a>
            <p><u>Tianxin Wei</u>, Bowen Jin, Ruirui Li, Hansi Zeng, Zhengyang Wang, Jianhui Sun, Qingyu Yin, Hanqing Lu, Suhang Wang, Jingrui He, Xianfeng Tang</p>
            <p><em>ICLR 2024 (Full Research, AR: 31%).</em> <a href="https://github.com/weitianxin/UniMP" class="button">Code</a></p>
            <img src="images/tag-multimodal.png" width="9%"> <img src="images/tag-transferability.png" width="15%">

          <li> <a href="https://arxiv.org/pdf/2311.09134.pdf"><strong>Scalable and Effective Generative Information Retrieval</strong></a>
            <p>Hansi Zeng, Chen Luo, Bowen Jin, Sheikh Muhammad Sarwar, <u>Tianxin Wei</u>, Hamed Zamani</p>
            <p><em>WWW 2024 (Oral, Full Research, AR: 20.2%).</em> <a href="https://github.com/hansizeng/ripor" class="button">Code</a></p>
            <img src="images/tag-retrieval.png" width="13%">

          <li> <a href="TBD"><strong>TAU: Trajectory Data Augmentation with Uncertainty for Next POI Recommendation</strong></a>
                      <p>Zhuang Zhuang, <u>Tianxin Wei</u>^, Lingbo Liu, Heng Qi, Yanming Shen, Baocai Yin</p>
                      <p><em>AAAI 2024 (Full Research, AR: 24%).</em> <a href="TBD" class="button">Code</a></p>
                      <img src="images/tag-robustness.png" width="14.5%">      <img src="images/tag-application.png" width="11%">

          <li> <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/c9e6ac15e689e06139d7b39e1667b165-Abstract-Conference.html"><strong>Meta-Learning with Neural Bandit Scheduler</strong></a>
                      <p>Yunzhe Qi, Yikun Ban, <u>Tianxin Wei</u>, Jiaru Zou, Huaxiu Yao, Jingrui He</p>
                      <p><em>NeurIPS 2023 (Full Research, AR: 26.1%).</em> <a href="https://github.com/yunzhe0306/bandit_task_scheduler" class="button">Code</a></p>
                      <img src="images/tag-sample_efficiency.png" width="12.5%">

          <li> <a href="https://arxiv.org/abs/2310.18816"><strong>Adaptive Test-Time Personalization for Federated Learning</strong></a>
                      <p>Wenxuan Bao*, <u>Tianxin Wei</u>*, Haohan Wang, Jingrui He</p>
                      <p><em>NeurIPS 2023 (Full Research, AR: 26.1%).</em> <a href="https://github.com/baowenxuan/ATP"  class="button">Code</a></p>
                      <img src="images/tag-robustness.png" width="14.5%">

          <li> <a href="https://arxiv.org/abs/2307.08941v2"><strong>NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning</strong></a>
                      <p><u>Tianxin Wei</u>*^, Zeming Guo*, Yifan Chen*, Jingrui He</p>
                      <p><em>ICML 2023 (Full Research, AR: 27.9%).</em> <a href="https://github.com/weitianxin/mlp_fusion" class="button">Code</a></p>
                      <img src="images/tag-model_efficiency.png" width="12.5%">

          <li> <a href="https://arxiv.org/abs/2210.03801"><strong>Augmentations in Hypergraph Contrastive Learning: Fabricated and Generative</strong></a>
                      <p><u>Tianxin Wei</u>*, Yuning You*, Tianlong Chen, Yang Shen, Jingrui He, Zhangyang Wang</p>
                      <p><em>NeurIPS 2022 (Full Research, AR: 25.6%).</em> <a href="https://github.com/weitianxin/HyperGCL" class="button">Code</a> <a href="https://weitianxin.github.io/files/neurips22_hypergcl_appendix.pdf" class="button">Appendix</a></p>
                      <img src="images/tag-robustness.png" width="14.5%">

          <li> <a href="https://arxiv.org/abs/2206.04789"><strong>Comprehensive Fair Meta-learned Recommender System</strong></a>
                      <p><u>Tianxin Wei</u>, Jingrui He</p>
                      <p><em>KDD 2022 (Full Research, AR: 15.0%).</em> <a href="https://github.com/weitianxin/MACR" class="button">Code</a></p>
                      <img src="images/tag-fairness.png" width="14.5%">

          <li> <a href="https://arxiv.org/abs/2010.15363"><strong>Model-Agnostic Counterfactual Reasoning for Eliminating Popularity Bias in Recommender System</strong></a>
                      <p><u>Tianxin Wei</u>, Fuli Feng, Jiawei Chen, Ziwei Wu, Jinfeng Yi, Xiangnan He</p>
                      <p><em>KDD 2021 (Full Research, AR: 15.4%).</em> <a href="https://github.com/weitianxin/MACR" class="button">Code</a></p>
                      <img src="images/tag-bias.png" width="14.5%"> 

          <li> <a href="https://arxiv.org/abs/2105.06067"><strong>Causal Intervention for Leveraging Popularity Bias in Recommendation</strong></a>
                      <p>Yang Zhang, Fuli Feng, Xiangnan He, <u>Tianxin Wei</u>, Chonggang Song, Guohui Ling and Yongdong Zhang</p>
                      <p><em>SIGIR 2021 (Best Paper Honorable Mention, 1 out of All, Full Research, AR: 21%).</em> <a href="https://github.com/zyang1580/PDA" class="button">Code</a></p>
                      <img src="images/tag-bias.png" width="14.5%">      <img src="images/tag-application.png" width="11%">

          <li> <a href="https://weitianxin.github.io/files/ICDM_2020_MetaCF.pdf"><strong>Fast Adaptation for Cold-start Collaborative Filtering with Meta-learning</strong></a>
                      <p><u>Tianxin Wei</u>, Ziwei Wu, Ruirui Li, Ziniu Hu, Fuli Feng, Xiangnan He, Yizhou Sun, and Wei Wang</p>
                      <p><em>ICDM 2020 (Full Research, AR: 9.8%).</em> <a href="https://drive.google.com/file/d/1_UaPcCQLaEEWUCsMTRIgsvtWqorqsnUm/view?usp=sharing" class="button">Code</a></p>
                      <img src="images/tag-transferability.png" width="15%">      <img src="images/tag-application.png" width="11%">

          <li> <a href="https://www.springerprofessional.de/en/unpaired-multimodal-neural-machine-translation-via-reinforcement/19040758"><strong>Unpaired Multimodal Neural Machine Translation via Reinforcement Learning</strong></a>
                      <p>Yijun Wang*, <u>Tianxin Wei</u>*, Qi Liu, Enhong Chen</p>
                      <p><em>DASFAA 2021 (Full Research, AR: 20%).</em></p>
                      <img src="images/tag-multimodal.png" width="9%">

          <li> <a href="https://weitianxin.github.io/files/AAAI21_ARStock.pdf"><strong>AR-Stock: Deep Augmented Relational Stock Prediction</strong></a>
                      <p><u>Tianxin Wei</u>, Yuning You, Tianlong Chen</p>
                      <p><em>AAAI 2021 Workshop on Knowledge Discovery from Unstructured Data (Oral).</em> <a href="https://github.com/weitianxin/FiAI_AR-Stock" class="button">Code</a></p>
                      <img src="images/tag-application.png" width="11%">
        </ul>
        &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<strong>Journal</strong>
        <br>
        <br>
        <ul>
          <li>
            <a href="http://sites.computer.org/debull/A23june/p80.pdf"><strong>Graph Contrastive Learning: An Odyssey towards Generalizable, Scalable and Principled Representation Learning on Graphs
            </strong></a>
                      <p>Yan Han, Yuning You, Wenqing Zheng, Scott Hoang, <u>Tianxin Wei</u>, Majdi Hassan, Tianlong Chen, Ying Ding, Yang Shen, Zhangyang Wang                      </p>
                      <p><em>IEEE Data Engineering Bulletin.</em></p>
                      <img src="images/tag-survey.png" width="8.5%">

        </ul>

<!--         &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<strong>Preprint</strong>
        <br>
        <br>
         

        <ul>
          <li>
            <a href="TBD"><strong>Space-efficient MoE Module Approximation via Wasserstein Barycenter and Residual Restoration</strong></a>
            <p><em>preprint 2024.</em> <a href="TBD" class="button">Code</a></p>
            <img src="images/tag-model_efficiency.png" width="12.5%">


          <li>
            <a href="TBD"><strong>Robust Watermarking for Diffusion Models</strong></a>
            <p><em>preprint 2024.</em> <a href="TBD" class="button">Code</a></p>
            <img src="images/tag-safety.png" width="13.5%">

          <li>
            <a href="TBD"><strong>Automatic Optimizer for Black-box LLMs</strong></a>
            <p><em>preprint 2024.</em> <a href="TBD" class="button">Code</a></p>
            <img src="images/tag-application.png" width="11%">
          
          <li>
            <a href="https://arxiv.org/pdf/2410.06467"><strong>WAPITI: A Watermark for Finetuned Open-Source LLMs</strong></a>
            <p>Lingjie Chen, Ruizhong Qiu, Siyu Yuan, Zhining Liu, <u>Tianxin Wei</u>, Hyunsik Yoo, Zhichen Zeng, Deqing Yang, Hanghang Tong</p>
            <p><em>preprint 2024.</em> <a href="TBD" class="button">Code</a></p>
            <img src="images/tag-safety.png" width="13.5%">

          <li>
            <a href="https://arxiv.org/pdf/2410.11235"><strong>Unleashing the Power of LLMs as Multi-Modal Encoders for Text and Graph-Structured Data</strong></a>
            <p>Jiacheng Lin, Kun Qian, Haoyu Han, Nurendra Choudhary, <u>Tianxin Wei</u>, Zhongruo Wang, Sahika Genc, Edward W Huang, Sheng Wang, Karthik Subbian, Danai Koutra, Jimeng Sun</p>
            <p><em>preprint 2024.</em> <a href="TBD" class="button">Code</a></p>
            <img src="images/tag-multimodal.png" width="9%">
        </ul> -->
        
        
        
        
                  <!-- honors and awards -->  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" id="section6">
          <tbody><tr>
            <td width="100%" valign="middle">
              <heading>Services & Awards</heading>
              <hr>
              <br>
              <strong>KDD 2025 Outstanding Reviewer</strong><br>
              <strong>University Nomination (Top3) for Apple Scholar in AI/ML 2024</strong><br>
              <strong>Amazon Internship Fellowship in 2024 ($40k)</strong><br>
              <strong>Conference Presentation Award, UIUC 2023</strong><br>
              <strong>SIGIR 2021 Best Paper Honorable Mention</strong><br>
              <strong>NeurIPS 2023 Scholar Award</strong><br>
              <strong>NeurIPS 2022 Scholar Award</strong><br>
              <strong>ICML 2023 Grant Award</strong><br>
              <strong>Program Committee/Reviewer: ICML (2021-2025), NeurIPS (2022-2025), ICLR (2023-2025), KDD (2023-2025), AAAI (2023-2024), CIKM (2021-2023), WSDM 2023, ACL 2023, EMNLP 2023, LOG 2022 <br>
              <strong>Journal Reviewer: TPAMI, TKDD, TOIS, TKDE, DMKD, Machine Learning, TMLR</strong><br>
            </td>
          </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
                <tr>
                    <td width="100%" valign="middle">
                        <heading>Pets</heading>
                        <hr>
                        <!-- <br> -->
                        <!-- Tuantuan & Qiaoqiao -->
                    </td>
                </tr>
                <tr>
                    <td style="text-align: center;">
                        <img src="images/pet2.jpg" class="img-fluid z-depth-1 rounded-circle" width="28%" style="display: inline-block;">
                        &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
                        <img src="images/pet1.jpg" class="img-fluid z-depth-1 rounded-circle" width="28%" style="display: inline-block;">

                      </td>
                </tr>
                
            </tbody>
        </table>
        <script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5w8l886z2zy&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script>

        <br>
        <br>

        <!-- Education -->
       <script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        <script type="text/javascript">
          try {
            var pageTracker = _gat._getTracker("UA-7580334-1");
            pageTracker._trackPageview();
          } catch (err) {}
        </script>
        </td>
    </tr>
  </table>

</body>

</html>
